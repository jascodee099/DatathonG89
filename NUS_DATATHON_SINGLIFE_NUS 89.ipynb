    %pip install pandas 
    %pip install matplotlib
    %pip install pandas 
    %pip install pyarrow
    %pip install numpy
    %pip install scikit-learn
    %pip install imbalanced-learn
    %pip install seaborn
        
    # Can have as many cells as you want for code
    
    import pandas as pd
    filepath = "./data/catB_train.parquet" 
    # the initialised filepath MUST be a relative path to a folder named data that contains the parquet file

    import pandas as pd
    import numpy as np

    df = pd.read_parquet("./data/catB_train.parquet")
    df.head()
    df.isna().sum()
    thresh = 50

    col = "race_desc"
    print(f"Column '{col}' is mostly empty:" , df[col].isna().sum()/df.shape[0]*100>thresh)
    col = "ctrycode_desc"
    print(f"Column '{col}' is mostly empty:" , df[col].isna().sum()/df.shape[0]*100>thresh)
    col = 'flg_gi_claim_29d435_ever'
    print(f"Column '{col}' is mostly empty:" , df[col].isna().sum()/df.shape[0]*100>thresh)
    col = 'flg_gi_claim_058815_ever'
    print(f"Column '{col}' is mostly empty:" , df[col].isna().sum()/df.shape[0]*100>thresh)
    col = 'flg_gi_claim_42e115_ever'
    print(f"Column '{col}' is mostly empty:" , df[col].isna().sum()/df.shape[0]*100>thresh)
    col = 'flg_gi_claim_856320_ever'
    print(f"Column '{col}' is mostly empty:" , df[col].isna().sum()/df.shape[0]*100>thresh)
    columns_to_delete = ['flg_gi_claim_29d435_ever','flg_gi_claim_058815_ever','flg_gi_claim_42e115_ever','flg_gi_claim_856320_ever']
    df.drop(columns = columns_to_delete, inplace = True)

    df["f_purchase_lh"] = df["f_purchase_lh"].fillna(0)
    #identifiy numeric columns and fill null values with the median value
    numeric_cols = df.select_dtypes(include=["int64", "float64"]).columns
    df[numeric_cols] = df[numeric_cols].apply(lambda x: x.fillna(x.median()))
    #Find out which are the non-numeric cols
    non_numeric_cols = df.select_dtypes(include=["string", "object"]).columns

    # to delete columns containing only 0s
    df[numeric_cols].mean()[(df[numeric_cols].min() == 0) & (df[numeric_cols].max() == 0)].head()
    # to find out which columns contained only 0s
    df = df.drop(columns = ['is_dependent_in_at_least_1_policy','f_hold_d0adeb','f_hold_gi','f_ever_bought_d0adeb','f_ever_bought_ltc_1280bf'])
    # detects and deletes columns with just one unique value 
    for c in df.columns:
        if len(df[c].unique()) == 1:
            df = df.drop(columns = c)

    # Ordinal Encoding

    from sklearn.preprocessing import OrdinalEncoder, LabelEncoder 
    # Identify non-numeric columns for ordinal encoding
    categorical_columns = df.select_dtypes(include=['object','string']).columns

    # Use OrdinalEncoder for ordinal encoding on non-numeric columns
    ordinal_encoder = OrdinalEncoder()
    df[categorical_columns] = ordinal_encoder.fit_transform(df[categorical_columns])

    y = df["f_purchase_lh"]
    X = df.drop(columns=['f_purchase_lh'])

    #SMOTE
    from collections import Counter
    from imblearn.over_sampling import SMOTE
    from sklearn.model_selection import train_test_split

    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=0)

    print('Before:', Counter(y_train))
    X_train, y_train = SMOTE().fit_resample(X_train, y_train)
    print('After:', Counter(y_train))

    #Decision Tree Model
    from sklearn.model_selection import train_test_split

    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)
    from sklearn.tree import DecisionTreeClassifier, plot_tree

    dt_clf = DecisionTreeClassifier(max_leaf_nodes=10)
    dt_clf.fit(X_train, y_train)

    import matplotlib.pyplot as plt
    plt.figure(figsize=(200, 50))
    plot_tree(dt_clf, filled=True, feature_names=X_train.columns, class_names=df['f_purchase_lh'].unique().astype(str).tolist(), rounded=True)
    plt.show()

    #Heatmap
    import seaborn as sns
    import matplotlib.pyplot as plt

    # Create a subset with the specified features and the target variable
    subset = df[['n_months_last_bought_gi', 'is_consent_to_mail', 'is_class_1_2', 'f_purchase_lh']]

    # Calculate the correlation matrix
    correlation_matrix = subset.corr()

    # Plot the heatmap
    plt.figure(figsize=(8, 6))
    sns.heatmap(correlation_matrix[['f_purchase_lh']], annot=True, cmap='coolwarm', vmin=-1, vmax=1)
    plt.title('Correlation Heatmap with Target: f_purchase_lh')
    plt.show()

    #Cross Validation
    from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay

    # Evaluate on validation set
    y_val_pred = dt_clf.predict(X_val)
    print(classification_report(y_val, y_val_pred))

    # Code to generate visualisation of matrix
    cm = confusion_matrix(y_val, y_val_pred)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm,) # set display_label according to dataset
    disp.plot()


    from sklearn.model_selection import cross_val_score

    model = dt_clf
    scores = cross_val_score(model, X_train, y_train, cv=5)

    # By default, cross_val_score returns a numpy array of accurcacies for each train-test split
    #>>> scores
    #array([0.96..., 1. , 0.96..., 0.96..., 1. ])

    # To use other metrics for example, f1-macro score:
    from sklearn import metrics

    scores = cross_val_score(model,X_train, y_train, cv=5, scoring='f1_macro')
    scores

    #Gradient Boosting Classifier
    from sklearn.ensemble import GradientBoostingClassifier
    boosted_clf = GradientBoostingClassifier()
    boosted_clf.fit(X_train, y_train)
    print("train accuracy: ", boosted_clf.score(X_train, y_train))
    print("val accuracy: ", boosted_clf.score(X_val, y_val))
